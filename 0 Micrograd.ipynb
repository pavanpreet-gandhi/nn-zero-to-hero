{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Micrograd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a re-implementation of Micrograd for eductation purposes after watching [this](https://www.youtube.com/watch?v=VMj-3S1tku0&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=1) video."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "- We can represent mathematical functions as **computation graphs**. \n",
    "    - A computational graph is a directed acyclic graphs (DAG).\n",
    "    - Each node represents some variable.\n",
    "        - The leaf nodes *(a.k.a. source nodes)* represent the inputs to the function.\n",
    "        - The root node(s) *(a.k.a. sink nodes)* represents the output(s).\n",
    "        - The intermediate nodes represnt intermediate steps of the function.\n",
    "    - Each edge represents some mathematical operation (e.g sum, product, exponentiation).\n",
    "- **Backpropagation** is a method to compute the derivatives of parameters w.r.t. a functions output. To do this, we step backwards in the computation graph (using topoligical sort) and accumulate the gradients according to the **chain rule** from calculus.\n",
    "- **Neural networks** are just mathematical functions where the output is a prediction. We can compose a neural network with a loss function to calculate the **loss**.\n",
    "- To train a neural network we must compute the **derivatives of all the learnable parameters** w.r.t. the loss, this can be done with backpropagation.\n",
    "- **Autograd** is a backpropagation engine used by PyTorch to build computational graphs and compute the derivatives of the leaf nodes w.r.t. the output (usually a loss function). It is optimized to work with tensors and take advantage of parallel computing (using GPUs).\n",
    "- **Note:** Backpropagation is more general and can apply to any mathematical function. It just happens to be very useful for training neural networks.\n",
    "\n",
    "In this notebook, we build **micrograd**, a simplified version of autograd at the scalar level for eductation purposes. The API of the gradient engine and the neuron class is very similar to the PyTorch API."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extras\n",
    "Most common deep-learning mistakes:\n",
    "1. You didn't try to overfit a single batch first. \n",
    "2. You forgot to toggle train/eval mode for the net. \n",
    "3. You forgot to `.zero_grad()` (in pytorch) before `.backward()`. \n",
    "4. You passed softmaxed outputs to a loss that expects raw logits.\n",
    "5. You didn't use `bias=False` for your Linear/Conv2d layer when using BatchNorm, or conversely forget to include it for the output layer.\n",
    "6. Thinking `view()` and `permute()` are the same thing (& incorrectly using view)\n",
    "\n",
    "[Defining new autograd functions in PyTorch](https://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_custom_function.html#pytorch-defining-new-autograd-functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scalar:\n",
    "    def __init__(self, value: float, children: set = set(), operation: str = \"None\"):\n",
    "        self.value: float = value\n",
    "        self.grad: float = 0.0\n",
    "        self.children: set = children\n",
    "        self.operation: str = operation\n",
    "        self._backward: function = lambda: None\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Scalar(value={self.value}, grad={self.grad})\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"\\nScalar(value={self.value}, grad={self.grad}, children={self.children}, operation={self.operation})\"\n",
    "\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Scalar) else Scalar(other)\n",
    "        out = Scalar(self.value + other.value, set((self, other)), \"+\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "\n",
    "    def __neg__(self):  # -self\n",
    "        return self * -1\n",
    "\n",
    "    def __sub__(self, other):  # self - other\n",
    "        return self + (-other)\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Scalar) else Scalar(other)\n",
    "        out = Scalar(\n",
    "            value=self.value * other.value, children=set((self, other)), operation=\"*\"\n",
    "        )\n",
    "\n",
    "        # Product rule + Chain rule\n",
    "        def _backward():\n",
    "            self.grad += other.value * out.grad\n",
    "            other.grad += self.value * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __rmul__(self, other):\n",
    "        return self * other\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        other = other if isinstance(other, Scalar) else Scalar(other)\n",
    "        out = Scalar(\n",
    "            self.value / other.value, children=set((self, other)), operation=\"/\"\n",
    "        )\n",
    "\n",
    "        # Quotient rule + Chain rule\n",
    "        def _backward():\n",
    "            self.grad += (1.0 / other.value) * out.grad\n",
    "            other.grad += (-self.value / other.value**2) * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def exp(self):\n",
    "        out = Scalar(math.exp(self.value), children=set((self,)), operation=\"exp\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += out.value * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def log(self):\n",
    "        out = Scalar(math.log(self.value), children=set((self,)), operation=\"log\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (1.0 / self.value) * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def sigmoid(self):\n",
    "        out = Scalar(\n",
    "            1 / (1 + math.exp(-self.value)), children=set((self,)), operation=\"sigmoid\"\n",
    "        )\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += out.value * (1.0 - out.value) * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self):\n",
    "        # Topologically sort the computational graph (DAG)\n",
    "        topo = []\n",
    "        visited = set()\n",
    "\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v.children:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "\n",
    "        build_topo(self)\n",
    "\n",
    "        # Backpropagate through the computational graph to accumilate gradients\n",
    "        self.grad = 1.0\n",
    "        for v in reversed(topo):\n",
    "            v._backward()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Engine Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(242.40610125372064, -253.43468221607841)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Scalar(-4.0)\n",
    "z = 2 * x + 2 + x\n",
    "q = z.sigmoid() + z * x\n",
    "h = (z * z).sigmoid()\n",
    "y = h + q + q.log() / x.exp()\n",
    "y.backward()\n",
    "y.value, x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(242.40610125372064, -253.43468221607847)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.Tensor([-4.0]).double()\n",
    "x.requires_grad = True\n",
    "z = 2 * x + 2 + x\n",
    "q = z.sigmoid() + z * x\n",
    "h = (z * z).sigmoid()\n",
    "y = h + q + q.log() / x.exp()\n",
    "y.backward()\n",
    "y.data.item(), x.grad.item()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, inputs: int, activation: bool = True):\n",
    "        self.inputs, self.activation = inputs, activation\n",
    "        self.weights = [Scalar(random.uniform(-1, 1)) for _ in range(self.inputs)]\n",
    "        self.bias = Scalar(0)\n",
    "\n",
    "    def __call__(self, x: list[Scalar]):\n",
    "        out = sum([w * xi for w, xi in zip(self.weights, x)]) + self.bias\n",
    "        if self.activation:\n",
    "            out = out.sigmoid()\n",
    "        return out\n",
    "\n",
    "    def parameters(self):\n",
    "        out = self.weights + [self.bias]\n",
    "        return out\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Neuron(inputs={self.inputs}, activation={self.activation})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Scalar(value=0.4925157220764906, grad=0.0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "n = Neuron(5, activation=True)\n",
    "x = [Scalar(1) for _ in range(5)]\n",
    "n(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, inputs: int, outputs: int, softmax: bool = False):\n",
    "        self.inputs, self.outputs, self.softmax = inputs, outputs, softmax\n",
    "        self.neurons = [\n",
    "            Neuron(inputs, activation=(not softmax)) for _ in range(outputs)\n",
    "        ]\n",
    "\n",
    "    def __call__(self, x: list[Scalar]):\n",
    "        out = [neuron(x) for neuron in self.neurons]\n",
    "        if self.softmax:\n",
    "            denom = sum([s.exp() for s in out])\n",
    "            out = [s.exp() / denom for s in out]\n",
    "        return out[0] if len(out) == 1 else out\n",
    "\n",
    "    def parameters(self):\n",
    "        out = [param for neuron in self.neurons for param in neuron.parameters()]\n",
    "        return out\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Layer(inputs={self.inputs}, outputs={self.outputs}, softmax={self.softmax})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum: 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Scalar(value=0.3419307179578387, grad=0.0),\n",
       " Scalar(value=0.08672889872577426, grad=0.0),\n",
       " Scalar(value=0.5713403833163871, grad=0.0)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "L = Layer(5, 3, softmax=True)\n",
    "x = [Scalar(1) for _ in range(10)]\n",
    "print(f\"Sum: {sum([v.value for v in L(x)])}\")\n",
    "L(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, structure: list[int], final_layer_softmax=True):\n",
    "        self.structure = structure\n",
    "        assert len(structure) >= 2\n",
    "        self.layers = [\n",
    "            Layer(inputs, outputs, softmax=False)\n",
    "            for inputs, outputs in zip(structure[:-2], structure[1:])\n",
    "        ]\n",
    "        self.layers += [\n",
    "            Layer(structure[-2], structure[-1], softmax=final_layer_softmax)\n",
    "        ]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        out = x\n",
    "        for layer in self.layers:\n",
    "            out = layer(out)\n",
    "        return out\n",
    "\n",
    "    def parameters(self):\n",
    "        out = [param for layer in self.layers for param in layer.parameters()]\n",
    "        return out\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"MLP(structure={self.structure})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum: 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Scalar(value=0.3969758907525013, grad=0.0),\n",
       " Scalar(value=0.19735471920930067, grad=0.0),\n",
       " Scalar(value=0.4056693900381981, grad=0.0)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "mlp = MLP([5, 10, 10, 3])\n",
    "x = [Scalar(1) for _ in range(5)]\n",
    "print(f\"Sum: {sum([v.value for v in mlp(x)])}\")\n",
    "mlp(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_iris(return_X_y=True)\n",
    "\n",
    "X = [[Scalar(x) for x in row] for row in data[0]]\n",
    "print(f\"X.shape: ({len(X)}, {len(X[0])})\")\n",
    "\n",
    "\n",
    "# Helper function\n",
    "def label_to_list(label: int, num_labels: int) -> list[Scalar]:\n",
    "    out = [0 for _ in range(num_labels)]\n",
    "    out[label] = 1\n",
    "    out = [Scalar(x) for x in out]\n",
    "    return out\n",
    "\n",
    "\n",
    "Y = [label_to_list(label, 3) for label in data[1]]\n",
    "print(f\"Y.shape: ({len(Y)}, {len(Y[0])})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP([4, 6, 6, 3], final_layer_softmax=True)\n",
    "\n",
    "for epoch in range(1000):\n",
    "    \n",
    "    # forward pass\n",
    "    preds = [mlp(x) for x in X]\n",
    "    loss = -sum(\n",
    "        [sum([p.log() * t for p, t in zip(pred, targ)]) for pred, targ in zip(preds, Y)]\n",
    "    )\n",
    "\n",
    "    # backward pass\n",
    "    for p in mlp.parameters():\n",
    "        p.grad = 0.0\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    for p in mlp.parameters():\n",
    "        p.value += -0.002 * p.grad\n",
    "\n",
    "    print(f\"{epoch}\\t{loss.value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax(x):\n",
    "    return max(range(len(x)), key=lambda i: x[i])\n",
    "\n",
    "\n",
    "predictions = [argmax([s.value for s in row]) for row in [mlp(x) for x in X]]\n",
    "actual = data[1]\n",
    "\n",
    "print(f\"Accuracy: {sum(predictions==actual)/len(actual)*100:.1f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
