{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Architechture"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read it in to inspect it\n",
    "with open('tinyshakespeare.txt', 'r', encoding='utf-8') as f:\n",
    "  text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters: 1115394\n"
     ]
    }
   ],
   "source": [
    "print(\"length of dataset in characters:\", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "Vocab size: 65\n"
     ]
    }
   ],
   "source": [
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(\"Vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "itos = {i:ch for i,ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "print(encode(\"hii there\"))\n",
    "print(decode([46, 47, 47, 1, 58, 46, 43, 56, 43]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** This tokenize (encoder/decoder) converts each character into a unique number, however in practice people usually use **sub-word level encoders**. \n",
    "  - They have **larger vocabulary sizes**, but use **fewer numbers** to represent a given text.\n",
    "  - There is a **tradeoff** between vocabulary size and representation size. \n",
    "  - Popular **sub-word tokenizers** are [sentencepiece](https://github.com/google/sentencepiece) and [tiktoken](https://github.com/openai/tiktoken)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.shape=torch.Size([1115394])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
       "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
       "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
       "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
       "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
       "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.int64)\n",
    "print(f\"{data.shape=}\")\n",
    "data[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example sequence: tensor([18, 47, 56, 57, 58,  1, 15, 47, 58]) \n",
      "\n",
      "when input is tensor([18]) the target: 47\n",
      "when input is tensor([18, 47]) the target: 56\n",
      "when input is tensor([18, 47, 56]) the target: 57\n",
      "when input is tensor([18, 47, 56, 57]) the target: 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
     ]
    }
   ],
   "source": [
    "block_size = 8\n",
    "print(\"example sequence:\", train_data[:block_size+1], \"\\n\")\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "  context = x[:t+1]\n",
    "  target = y[t]\n",
    "  print(f\"when input is {context} the target: {target}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The transformer learns to predict the target given context of any length up to `block_size`. \n",
    "- This is not just for efficiency, and is infact necessery for inference since the transformer will need to be able to generate text from scratch (with no context, then context of length 1, and so on...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]], device='cuda:0')\n",
      "targets:\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # number of independant sequences we will process in paralell\n",
    "block_size = 8 # maximum context length for prediction\n",
    "\n",
    "def get_batch(data):\n",
    "  ix = torch.randint(len(data)-block_size, size=(batch_size,))\n",
    "  x = torch.stack([data[i : i+block_size] for i in ix])\n",
    "  y = torch.stack([data[i+1 : i+block_size+1] for i in ix])\n",
    "  x, y = x.to(device), y.to(device)\n",
    "  return x, y\n",
    "\n",
    "xb, yb = get_batch(train_data)\n",
    "print(f'inputs:')\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each batch contains $4 \\times 8$ training examples.\n",
    "- given `[24]` predict `43`\n",
    "- given `[24, 43]` predict `58`\n",
    "- etc...\n",
    "- given `[25, 17, 27, 10,  0, 21,  1, 54]` predict `39`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each batch has 2 dimensions. The first dimension is the *batch* dimension of size `4`, and the second dimension is the *time* dimension of size `8`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Neural Network\" Bigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "  \n",
    "  def __init__(self, vocab_size:int) -> None:\n",
    "    super().__init__()\n",
    "    # the embedding for each token is simply the logits for the next token\n",
    "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "  def forward(self, idx:torch.Tensor, targets:torch.Tensor|None = None) -> tuple[torch.Tensor, torch.Tensor|None]:\n",
    "    # idx and targets are both (B,T) tensors of integers\n",
    "    logits = self.token_embedding_table(idx) # (B, T, C)\n",
    "    \n",
    "    if targets is None:\n",
    "      loss = None\n",
    "    else:\n",
    "      B, T, C = logits.size()\n",
    "      # merge the batch and time dimension since F.cross_entropy expects only one \"batch\" dimension\n",
    "      loss = F.cross_entropy(logits.view(B*T, C), targets.view(B*T))\n",
    "    \n",
    "    return logits, loss\n",
    "  \n",
    "  def generate(self, idx:torch.Tensor, max_new_tokens:int) -> torch.Tensor:\n",
    "    # idx is (B,T) tensor of integers of current context\n",
    "    for _ in range(max_new_tokens):\n",
    "      logits, _loss = self(idx) # (B, T, C)\n",
    "      # we only care about the predicted token for the last time-step\n",
    "      logits = logits[:, -1, :] # (B, C)\n",
    "      probs = F.softmax(logits, dim=1)\n",
    "      idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "      idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "    return idx"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently the generator function is wildly inefficient for a simple bigram model since it remakes the predictions everytime for the full context when it just needs the previous word to predict the next word. However this is on purpose since in the next models we will need to use the context and this way we won't need to write a new generator function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits.shape=torch.Size([4, 8, 65])\n",
      "loss=tensor(4.8786, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "\n",
      "pYCXxfRkRZd\n",
      "wc'wfNfT;OLlTEeC K\n",
      "jxqPToTb?bXAUG:C-SGJO-33SM:C?YI3a\n",
      "hs:LVXJFhXeNuwqhObxZ.tSVrddXlaSZaNe\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "m = BigramLanguageModel(vocab_size).to(device)\n",
    "\n",
    "logits, loss = m(xb, yb)\n",
    "print(f\"{logits.shape=}\")\n",
    "print(f\"{loss=}\")\n",
    "\n",
    "context_idx = torch.zeros((1, 1), dtype=torch.int64).to(device)\n",
    "print(decode(m.generate(context_idx, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BigramLanguageModel(vocab_size).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3) # you can get away with larger lr for smaller networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.598329067230225\n",
      "3.657761812210083\n",
      "3.120805263519287\n",
      "2.6083383560180664\n",
      "2.5169241428375244\n",
      "2.612285614013672\n",
      "2.517780303955078\n",
      "2.4636025428771973\n",
      "2.619422197341919\n",
      "2.6189043521881104\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for step in range(10_000):\n",
    "  \n",
    "  xb, yb = get_batch(train_data)\n",
    "  logits, loss = model(xb, yb)\n",
    "  optimizer.zero_grad(set_to_none=True)\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "  \n",
    "  if step%1000==0:\n",
    "    print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wawice my.\n",
      "\n",
      "HDEdarom orou waowh$Frtof isth ble mil ndill, ath iree sengmin lat Heriliovets, and Win nghirileranousel lind me l.\n",
      "HAshe ce hiry:\n",
      "Supr aisspllw y.\n",
      "Hurindu n Boopetelaves\n",
      "MP:\n",
      "\n",
      "Pl, d mothakleo Windo whthCoribyo the m dourive we higend t so mower; te\n",
      "\n",
      "AN ad nterupt f s ar igr t m:\n",
      "\n",
      "Thiny aleronth,\n",
      "Mad\n",
      "Whed my o myr f-NLIERor,\n",
      "SS&Y:\n",
      "\n",
      "Sadsal thes ghesthidin cour ay aney Iry ts I fr y ce.\n",
      "Jken pand, bemary.\n",
      "Yor 'Wour menm sora anghy t-e nomes twe men.\n",
      "Wand thot sulin s th llety ome.\n",
      "I muc\n"
     ]
    }
   ],
   "source": [
    "context_idx = torch.zeros((1, 1), dtype=torch.int64).to(device)\n",
    "print(decode(model.generate(context_idx, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bigram model only takes the previous token as context. What we want is a model where all the previous tokes \"talk\" with each other and figure out the context that can be used to predict the next token. Enter transformers."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The mathematical trick in self-attention"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bag of words:** simply averaging all the previous words into one \"bag of words\" which is used for prediction at that time-step. The term \"bag of words\" almost always refers to some sort of **averaging**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 2\n",
    "x = torch.randn(B, T, C)\n",
    "x.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to create another tensor `xbow` with entries such that $\\text{xbow}[b, t] = \\text{mean}_{i \\leq t} \\left( \\text{x}[b, i] \\right)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 1\n",
    "xbow = torch.zeros(B, T, C)\n",
    "for b in range(B):\n",
    "  for t in range(T):\n",
    "    xbow[b, t] = x[b, :t+1].mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 2\n",
    "weights = torch.tril(torch.ones(T, T))\n",
    "weights = weights / weights.sum(dim=1, keepdim=True) # weights used to \"average\" just like above\n",
    "# print(f\"{weights=}\")\n",
    "xbow2 = weights @ x # extra B in weights from broadcasting: (B, T, T) @ (B, T, C) ---> (B, T, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 3\n",
    "affinities = torch.zeros((T, T)) # affinities between tokens (0 results in all tokens being equally weighted)\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "weights = affinities.masked_fill(tril==0, float('-inf')) # so that tokens from the future cannot communicate\n",
    "weights = F.softmax(weights, dim=1) # this gives us the same weight matrix as before\n",
    "xbow3 = weights @ x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(xbow, xbow2), torch.allclose(xbow2, xbow3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This illustrates a mathematical trick where we can \"average\" using matrix multiplication instead of using for loops. \n",
    "- This can also be extended to weighted averages very easily. Here the weights are normalized to sum to one and not look into the future of the time dimension.\n",
    "- *Version 3* is especially useful since it startes with \"affinities\" between different tokens, and they can be data dependant instead of just being set to zero."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crux of self attention"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Every token will emit three vectors - **query**, **key**, and **value**\n",
    "- The key tells other tokens what it *has*, and the query tells other tokens what it's *looking for*.\n",
    "- The value contains the *information* of the token that is relevant for other tokens.\n",
    "- The **dot product** between a key and query for a token pair become the *affinities* between tokens i.e. how interested is one token with another.\n",
    "- For example, a pronoun could be looking for a noun, so the dot product between the query of the pronoun and the key of the noun could be high.\n",
    "- The model then computes the weight matrix by performing softmax on the masked affinities.\n",
    "- This weight matrix is then multiplied with the values of all the tokens to produce the output of the self-attention layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0248, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0052, 0.0091, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0521, 0.0135, 0.2482, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3171, 0.0214, 0.1642, 0.1188, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0412, 0.0487, 0.1046, 0.0742, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1060, 0.5347, 0.2059, 0.1030, 0.7402, 0.0192, 0.0000, 0.0000],\n",
      "        [0.4298, 0.3409, 0.1769, 0.2027, 0.0480, 0.8472, 0.2329, 0.0000],\n",
      "        [0.0238, 0.0316, 0.1002, 0.5013, 0.0117, 0.1336, 0.7671, 1.0000]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([[0.0248],\n",
      "        [0.0143],\n",
      "        [0.3138],\n",
      "        [0.6216],\n",
      "        [0.4687],\n",
      "        [1.7090],\n",
      "        [2.2784],\n",
      "        [2.5694]], grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# version 4: self-attention!\n",
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 32 # C could be n_embd for example\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "# let's see a single head perform self-attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False) # just a simple matrix multiply layers\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x) # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "# each embedded token has produced a key and query independantly and in parllel - no communication has happened yet between tokens\n",
    "affinities = q @ k.transpose(-2, -1) # basically a batch dot product for every token-pair: (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "weights = affinities.masked_fill(tril==0, float('-inf')) # so that tokens from the future cannot communicate\n",
    "weights = F.softmax(weights, dim=1)\n",
    "  \n",
    "v = value(x) # (B, T, 16)\n",
    "out = weights @ v # (B, T, T) @ (B, T, 16) ---> (B, T, 16)\n",
    "\n",
    "out.shape\n",
    "print(weights[0])\n",
    "print(weights[0].sum(dim=1, keepdim=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
    "- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
    "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
    "- In an \"encoder\" attention block just delete the single line that does masking with `tril`, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
    "- \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
    "- \"Scaled\" attention additionally divides the `weights` by 1/sqrt(head_size). This makes it so when input `k`, `q` are unit variance, `weights` will be unit variance too and Softmax will stay diffuse and not saturate too much. This prevents Softmax from converging to one-hot vectors."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More notes:\n",
    "- **Multi-head attention**\n",
    "  - Often, the tokens often \"have a lot to talk about\". This communication can be facillitated by multiple attention heads between the same tokens.\n",
    "  - For example one of the heads could be looking for consonants, while the other could be looking for vowels, etc.\n",
    "  - The final output is a concatination between all the attention heads.\n",
    "- **Communication and computation**\n",
    "  - It is often useful to let the network **\"think over\"** the outputs of the attention blocks. This can be achieved by adding more **computation** steps using feed-forward layers. \n",
    "  - Infact a common architechure is to **intersperse** the attention blocks, that allow communication between tokens, with feed-forward blocks, that allow tokens to individually process the results of this communication.\n",
    "- **Skip/residual connections**\n",
    "  - Deep neural networks often suffer from optimization issues due to vanishing gradients. Skip connections (a.k.a. residual connections) help with this.\n",
    "  - The main concept is that you \"branch\" the data off to transform the data in some way, but also add the non-transformed version back to the \"main branch\". \n",
    "  - Since addition routes gradients, this creates a \"gradient super-highway\" that allows gradients to backpropagate more effectively to the earlier layers. \n",
    "  - The \"branches\" off the highway are where the computation happens, and they are initialized in such a way that they barely contribute any gradients, but eventually they become more relevant as training goes on.\n",
    "- **LayerNorm**\n",
    "  - This is another innovation that helps with numerical stability of the gradients during optimization of deep neural networks.\n",
    "  - The idea is the same as BatchNorm, we want every neuron to have unit gaussian outputs. \n",
    "  - However instead of normalizing over the batch dimension, we normalize over the time dimension. \n",
    "  - This is better than BatchNorm since batches are no longer coupled together and we don't need to keep track of training vs testing.\n",
    "- **Dropout**\n",
    "  - This is a regularization technique that helps prevent overfitting.\n",
    "  - Every forward pass, a random subset of neurons are disabled, so only a subset of the network is trained in each optimization step.\n",
    "  - This has the effect of training an ensemble of networks that are merged together at test time."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformer architechture has stood the test of time and has remained relatively unchanged for quite some time since it's introduction in 2017. One change that was made was the pre-norm formulation that adds a LayerNorm before the transformer block, and another one before the feed-forward block."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When we want the generated text to be conditioned on some previous text, we can use an encoder transformer blocks (without the traingular mask), to generate the keys and values, and a decoder transformer block to generate the queries.\n",
    "- These can be combined to form an encoder-decoder transformer. This is also known as cross-attention! \n",
    "- See the \"attention is all you need\" paper for more details"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to train ChatGPT\n",
    "### Stage 1: Pretraining\n",
    "- The first stage is to train a decoder transformer langauge model on all the data from the internet that simply learns to babble on\n",
    "- Here we trained a character level model with 10 million parameters on a dataset with 1 million character level tokens tokens which would be about 300,000 tokens in the OpenAI vocabulary.\n",
    "- The biggest OpenAI GPT 3 transformer has 175 billion parameters on 300 billion tokens. These numbers are not even large compared to current standards. See OpenAI's [GPT-3 paper](https://arxiv.org/abs/2005.14165) for more details.\n",
    "- Due to scale, this becomes a massive infrastructure challenge.\n",
    "### Stage 2: Fine-tuning\n",
    "- So far the pre-trained model will just autocomplete and babble pretty much anything. It could complete the sentence, it could write a news article, it could ask more questions, pretty much anything that it sees on the internet.\n",
    "- The next stage is to \"align it\" to be an assistant. See this [blog post](https://openai.com/blog/chatgpt) by OpenAI for more information.\n",
    "  - Step 1 fine-tunes this model based on a small dataset of good example \"question-answer\" pairs. This works because pre-trained LLMs are very sample efficient.\n",
    "  - Step 2 trains a reward model by asking humans to rank different responses from bad to good. This reward model is a way to score any response.\n",
    "  - Step 3 uses the PPO a reinforcement learning algorithm to further fine-tune the model based on the reward model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
