{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigrams and Ngrams"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro to Makemore\n",
    "- Makemore makes more of things that you give it. It take one text file as input where each newline contains a new training thing.\n",
    "- Under the hood, it is an autoregressive character-level language model, with a wide choice of models from bigrams all the way to a Transformer (exactly as seen in GPT).\n",
    "\n",
    "**Progression:** Character-level langauge model (this) $\\rightarrow$ Word-level langauge model (to generate documents) $\\rightarrow$ Image-text networks (e.g Dalle, Stable Diffusion, etc.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extras:**\n",
    "- [PyTorch Broadcasting Semantics](https://pytorch.org/docs/stable/notes/broadcasting.html#:~:text=Broadcasting%20semantics%201%20General%20semantics%20Two%20tensors%20are,broadcast.%20For%20Example%3A%20...%203%20Backwards%20compatibility%20)\n",
    "- [Maximum Likelihood Estimation](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation):\n",
    "    - MLE is a method of estimating the parameters of an assumed probability distribution, given some observed data. \n",
    "    - This is achieved by maximizing a **likelihood function** so that, under the assumed statistical model, the observed data is most probable.\n",
    "    - The logic of maximum likelihood is both intuitive and flexible, and as such the method has become a dominant means of statistical inference."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(linewidth=140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open(\"names.txt\", \"r\").read().splitlines()\n",
    "words[:5], len(words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram Language Model\n",
    "Looks at the previous word to predict the next word (very simplistic but a good place to start)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_counts = {}\n",
    "for w in words:\n",
    "    chs = [\"<S>\"] + list(w) + [\"<E>\"]  # adding special start and end characters\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):  # zip stops when either of the sequences end\n",
    "        bigram = (ch1, ch2)\n",
    "        # .get(_, 0) returns 0 if the bigram is not in the dictionary yet\n",
    "        bigram_counts[bigram] = bigram_counts.get(bigram, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .items() returns a dict_items object containing (key, value) tuples\n",
    "# sorted returns a new sorted list, key is how the items are sorted\n",
    "# we want to sort by value which is the second element in the tuple i.e item[1]\n",
    "sorted(bigram_counts.items(), key=lambda item: item[1], reverse=True)[:5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's more convenient to store these bigrams in a pytorch tensor where the first axis represents the first character, the second axis represents the second character, and the value represents the count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of all possible characters (we actually only need 1 special character for bigrams)\n",
    "chars = [\".\"] + sorted(set(\"\".join(words)))\n",
    "# mappings from characters to indices and vice-versa\n",
    "char_to_ix = {s: i for i, s in enumerate(chars)}\n",
    "ix_to_char = {i: s for s, i in char_to_ix.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = torch.zeros((len(chars), len(chars)), dtype=torch.int32)\n",
    "for w in words:\n",
    "    chs = [\".\"] + list(w) + [\".\"]  # adding special start and end characters\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):  # zip stops when either of the sequences end\n",
    "        ix1 = char_to_ix[ch1]\n",
    "        ix2 = char_to_ix[ch2]\n",
    "        N[ix1, ix2] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 14))\n",
    "plt.imshow(N, cmap=\"Blues\")\n",
    "for i in range(len(chars)):\n",
    "    for j in range(len(chars)):\n",
    "        chstr = ix_to_char[i] + ix_to_char[j]\n",
    "        plt.text(j, i, chstr, ha=\"center\", va=\"bottom\", color=\"black\")\n",
    "        plt.text(j, i, N[i, j].item(), ha=\"center\", va=\"top\", color=\"black\")\n",
    "plt.axis(\"off\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = N.float()\n",
    "# normalising the distribution is not necessary since torch.multinomial can take weights as input\n",
    "P /= P.sum(dim=1, keepdims=True)\n",
    "# note:\n",
    "# P /= ... is an inplace operation (more efficient since doesn't use more memory)\n",
    "# P = P/... is not an inplace operatoion\n",
    "\n",
    "# set the random number generator for reproducability\n",
    "g = torch.Generator().manual_seed(5)\n",
    "for _ in range(20):\n",
    "    ix = 0\n",
    "    while True:\n",
    "        ix = P[ix].multinomial(num_samples=1, replacement=True, generator=g).item()\n",
    "        char = ix_to_char[ix]\n",
    "        if ix == 0:\n",
    "            print(\" \", end=\"\")\n",
    "            break\n",
    "        print(char, end=\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra: N-gram Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3\n",
    "# model smoothing: add a small amount to prevent infinite negative log likelihood loss\n",
    "N_n = torch.zeros([len(chars) for _ in range(n)], dtype=float) + 0.001\n",
    "for w in words:\n",
    "    chs = [\".\" for _ in range(n-1)] + list(w) + [\".\"]  # adding special start and end characters\n",
    "    # zip stops when either of the sequences end\n",
    "    for chs in zip(*[chs[i:] for i in range(n)]):\n",
    "        ixs = tuple([char_to_ix[char] for char in chs])\n",
    "        N_n[ixs] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the random number generator for reproducability\n",
    "g = torch.Generator().manual_seed(5)\n",
    "P_n = N_n / N_n.sum(dim=-1, keepdim=True) # not necessery but useful later\n",
    "for _ in range(20):\n",
    "    ix = (0,)*(n-1)\n",
    "    while True:\n",
    "        i = P_n[ix].multinomial(num_samples=1, replacement=True, generator=g).item()\n",
    "        ix = (*ix[1:], i)\n",
    "        char = ix_to_char[i]\n",
    "        if i == 0:\n",
    "            print(\" \", end=\"\")\n",
    "            break\n",
    "        print(char, end=\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Higher values of `n` result in crazy sizes for the Ngram table, so this appeoach is not scalable to take in long inputs when predicting the next word."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Likelihood\n",
    "This is the **probability of the entire dataset according to the model** (i.e. what is the probability of coming accross the dataset given that our model represents the true distribution).\n",
    "-  **GOAL:** maximize likelihood of the data w.r.t. model parameters (statistical modeling)\n",
    "-  equivalent to maximizing the log likelihood (because log is monotonic)\n",
    "-  equivalent to minimizing the negative log likelihood\n",
    "-  equivalent to minimizing the average negative log likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average negative log likelihood of bigram model\n",
    "log_likelihood, count = 0, 0\n",
    "\n",
    "for w in words[:]:\n",
    "    chs = [\".\"] + list(w) + [\".\"]  # adding special start and end characters\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):  # zip stops when either of the sequences end\n",
    "        ix1, ix2 = char_to_ix[ch1], char_to_ix[ch2]\n",
    "        prob = P[ix1, ix2]\n",
    "        logprob = torch.log(prob)\n",
    "        log_likelihood += logprob\n",
    "        count += 1\n",
    "        # print(f\"{ch1}{ch2}\\t{prob:.4f}\\t{logprob:.4f}\")\n",
    "\n",
    "print(f\"{log_likelihood=}\")\n",
    "negative_log_likelihood = -log_likelihood\n",
    "print(f\"{negative_log_likelihood=}\")\n",
    "print(f\"{negative_log_likelihood/count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average negative log likelihood of Ngram model\n",
    "log_likelihood, count = 0, 0\n",
    "\n",
    "for w in words[:]:\n",
    "    chs = [\".\" for _ in range(n-1)] + list(w) + [\".\"]\n",
    "    for chs in zip(*[chs[i:] for i in range(n)]):\n",
    "        ixs = tuple([char_to_ix[char] for char in chs])\n",
    "        prob = P_n[ixs]\n",
    "        logprob = torch.log(prob)\n",
    "        log_likelihood += logprob\n",
    "        count += 1\n",
    "        # print(f\"{ch1}{ch2}\\t{prob:.4f}\\t{logprob:.4f}\")\n",
    "\n",
    "print(f\"{log_likelihood=}\")\n",
    "negative_log_likelihood = -log_likelihood\n",
    "print(f\"{negative_log_likelihood=}\")\n",
    "print(f\"{negative_log_likelihood/count} <- lower!\") # but did it overfit? no, it clearly has not memorized the data, the model itself is too small."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
